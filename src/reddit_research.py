#!/usr/bin/env python3
"""
Redditè®ºå›éœ€æ±‚ç ”ç©¶è„šæœ¬
ç”¨äºè‡ªåŠ¨é‡‡é›†å’Œåˆ†æç”µå•†/å¹¿å‘Šç›¸å…³subredditçš„ç”¨æˆ·ç—›ç‚¹
"""

import praw
import pandas as pd
from datetime import datetime, timedelta
import re
from collections import Counter
import json
import time
from textblob import TextBlob
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class RedditResearcher:
    def __init__(self):
        """åˆå§‹åŒ–Reddit APIè¿æ¥"""
        # éœ€è¦åœ¨ https://www.reddit.com/prefs/apps åˆ›å»ºåº”ç”¨è·å–å‡­è¯
        self.reddit = praw.Reddit(
            client_id=os.getenv('REDDIT_CLIENT_ID', 'your_client_id'),
            client_secret=os.getenv('REDDIT_CLIENT_SECRET', 'your_client_secret'),
            user_agent='AdDAO Research Bot 1.0',
            username=os.getenv('REDDIT_USERNAME'),
            password=os.getenv('REDDIT_PASSWORD')
        )
        
        # ç›®æ ‡subreddits
        self.target_subreddits = [
            'shopify',
            'ecommerce', 
            'dropshipping',
            'FacebookAds',
            'PPC',
            'AmazonSeller',
            'FulfillmentByAmazon'
        ]
        
        # ç—›ç‚¹å…³é”®è¯
        self.pain_keywords = {
            'competitor_spying': [
                'spy on competitors', 'competitor ads', 'what ads running',
                'competitive analysis', 'competitor research', 'spy tools'
            ],
            'ad_cost': [
                'ads expensive', 'high cpc', 'rising costs', 'budget waste',
                'cpm increase', 'roas decrease', 'losing money'
            ],
            'optimization': [
                'optimize ads', 'improve performance', 'better roas',
                'conversion rate', 'ad fatigue', 'creative ideas'
            ],
            'analytics': [
                'track metrics', 'understand data', 'analytics confusing',
                'which metrics', 'reporting', 'dashboard'
            ],
            'targeting': [
                'audience targeting', 'find customers', 'interests',
                'lookalike', 'custom audience', 'retargeting'
            ]
        }
        
        # è§£å†³æ–¹æ¡ˆå…³é”®è¯
        self.solution_keywords = [
            'tool', 'software', 'app', 'platform', 'service',
            'recommend', 'using', 'tried', 'works well'
        ]
        
    def search_posts(self, days_back=30, limit_per_sub=100):
        """æœç´¢æœ€è¿‘Nå¤©çš„ç›¸å…³å¸–å­"""
        all_posts = []
        
        for subreddit_name in self.target_subreddits:
            print(f"\næ­£åœ¨æœç´¢ r/{subreddit_name}...")
            try:
                subreddit = self.reddit.subreddit(subreddit_name)
                
                # æœç´¢å¤šä¸ªæ—¶é—´èŒƒå›´çš„å¸–å­
                for sort_by in ['hot', 'new', 'top']:
                    posts = []
                    
                    if sort_by == 'hot':
                        posts = subreddit.hot(limit=limit_per_sub)
                    elif sort_by == 'new':
                        posts = subreddit.new(limit=limit_per_sub)
                    elif sort_by == 'top':
                        posts = subreddit.top('month', limit=limit_per_sub)
                    
                    for post in posts:
                        # è¿‡æ»¤æ—¶é—´
                        post_time = datetime.fromtimestamp(post.created_utc)
                        if post_time < datetime.now() - timedelta(days=days_back):
                            continue
                        
                        # æå–å¸–å­ä¿¡æ¯
                        post_data = {
                            'subreddit': subreddit_name,
                            'post_id': post.id,
                            'title': post.title,
                            'selftext': post.selftext,
                            'author': str(post.author),
                            'created_utc': post_time,
                            'score': post.score,
                            'num_comments': post.num_comments,
                            'url': f"https://reddit.com{post.permalink}",
                            'sort_by': sort_by
                        }
                        
                        # åˆ†æç—›ç‚¹
                        post_data['pain_points'] = self.analyze_pain_points(
                            post.title + ' ' + post.selftext
                        )
                        
                        # æƒ…æ„Ÿåˆ†æ
                        post_data['sentiment'] = self.analyze_sentiment(
                            post.title + ' ' + post.selftext
                        )
                        
                        all_posts.append(post_data)
                        
                # é¿å…APIé™åˆ¶
                time.sleep(2)
                
            except Exception as e:
                print(f"é”™è¯¯: {e}")
                continue
                
        return pd.DataFrame(all_posts)
    
    def search_comments(self, post_ids, limit=50):
        """è·å–æŒ‡å®šå¸–å­çš„è¯„è®º"""
        all_comments = []
        
        for post_id in post_ids[:limit]:  # é™åˆ¶æ•°é‡é¿å…APIè¶…é™
            try:
                submission = self.reddit.submission(id=post_id)
                submission.comments.replace_more(limit=0)  # å±•å¼€æ‰€æœ‰è¯„è®º
                
                for comment in submission.comments.list():
                    comment_data = {
                        'post_id': post_id,
                        'comment_id': comment.id,
                        'body': comment.body,
                        'author': str(comment.author),
                        'score': comment.score,
                        'created_utc': datetime.fromtimestamp(comment.created_utc)
                    }
                    
                    # æ£€æŸ¥æ˜¯å¦æåˆ°è§£å†³æ–¹æ¡ˆ
                    comment_data['mentions_solution'] = any(
                        keyword in comment.body.lower() 
                        for keyword in self.solution_keywords
                    )
                    
                    all_comments.append(comment_data)
                    
                time.sleep(1)  # é¿å…APIé™åˆ¶
                
            except Exception as e:
                print(f"è·å–è¯„è®ºé”™è¯¯: {e}")
                continue
                
        return pd.DataFrame(all_comments)
    
    def analyze_pain_points(self, text):
        """åˆ†ææ–‡æœ¬ä¸­çš„ç—›ç‚¹"""
        text_lower = text.lower()
        found_pain_points = []
        
        for pain_type, keywords in self.pain_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                found_pain_points.append(pain_type)
                
        return found_pain_points
    
    def analyze_sentiment(self, text):
        """æƒ…æ„Ÿåˆ†æ"""
        try:
            blob = TextBlob(text)
            return {
                'polarity': blob.sentiment.polarity,  # -1åˆ°1
                'subjectivity': blob.sentiment.subjectivity  # 0åˆ°1
            }
        except:
            return {'polarity': 0, 'subjectivity': 0}
    
    def extract_price_mentions(self, text):
        """æå–ä»·æ ¼æåŠ"""
        price_pattern = r'\$\d+(?:\.\d{2})?(?:/(?:month|mo|year|yr))?'
        prices = re.findall(price_pattern, text)
        return prices
    
    def generate_report(self, posts_df, comments_df=None):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report = {
            'summary': {
                'total_posts': len(posts_df),
                'date_range': f"{posts_df['created_utc'].min()} to {posts_df['created_utc'].max()}",
                'subreddits_analyzed': posts_df['subreddit'].unique().tolist()
            },
            'pain_points': {},
            'sentiment_analysis': {},
            'top_posts': [],
            'price_insights': []
        }
        
        # ç—›ç‚¹ç»Ÿè®¡
        all_pain_points = []
        for pain_list in posts_df['pain_points']:
            all_pain_points.extend(pain_list)
        
        pain_counter = Counter(all_pain_points)
        report['pain_points'] = dict(pain_counter.most_common())
        
        # æƒ…æ„Ÿåˆ†æ
        avg_sentiment = posts_df['sentiment'].apply(lambda x: x['polarity']).mean()
        report['sentiment_analysis'] = {
            'average_sentiment': avg_sentiment,
            'negative_posts': len(posts_df[posts_df['sentiment'].apply(lambda x: x['polarity'] < -0.1)]),
            'positive_posts': len(posts_df[posts_df['sentiment'].apply(lambda x: x['polarity'] > 0.1)]),
            'neutral_posts': len(posts_df[posts_df['sentiment'].apply(lambda x: -0.1 <= x['polarity'] <= 0.1)])
        }
        
        # çƒ­é—¨å¸–å­ï¼ˆé«˜äº’åŠ¨ï¼‰
        top_posts = posts_df.nlargest(10, 'num_comments')[
            ['title', 'subreddit', 'score', 'num_comments', 'url', 'pain_points']
        ].to_dict('records')
        report['top_posts'] = top_posts
        
        # ä»·æ ¼æ´å¯Ÿ
        if comments_df is not None and len(comments_df) > 0:
            all_prices = []
            for text in comments_df['body']:
                prices = self.extract_price_mentions(text)
                all_prices.extend(prices)
            
            if all_prices:
                price_counter = Counter(all_prices)
                report['price_insights'] = dict(price_counter.most_common(10))
        
        return report
    
    def save_results(self, posts_df, comments_df, report):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # åˆ›å»ºç»“æœç›®å½•
        os.makedirs('research_results', exist_ok=True)
        
        # ä¿å­˜æ•°æ®
        posts_df.to_csv(f'research_results/posts_{timestamp}.csv', index=False)
        if comments_df is not None and len(comments_df) > 0:
            comments_df.to_csv(f'research_results/comments_{timestamp}.csv', index=False)
        
        # ä¿å­˜æŠ¥å‘Š
        with open(f'research_results/report_{timestamp}.json', 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self.generate_markdown_report(report, timestamp)
        
        return timestamp
    
    def generate_markdown_report(self, report, timestamp):
        """ç”Ÿæˆæ˜“è¯»çš„MarkdownæŠ¥å‘Š"""
        md_content = f"""# Redditç ”ç©¶æŠ¥å‘Š - {timestamp}

## æ¦‚è§ˆ
- åˆ†æå¸–å­æ•°ï¼š{report['summary']['total_posts']}
- æ—¶é—´èŒƒå›´ï¼š{report['summary']['date_range']}
- åˆ†æçš„subredditsï¼š{', '.join(report['summary']['subreddits_analyzed'])}

## ç—›ç‚¹åˆ†æ

### ç—›ç‚¹æ’å
"""
        for pain, count in report['pain_points'].items():
            md_content += f"- **{pain}**: {count}æ¬¡æåŠ\n"
        
        md_content += f"""
## æƒ…æ„Ÿåˆ†æ
- å¹³å‡æƒ…æ„Ÿå€¼ï¼š{report['sentiment_analysis']['average_sentiment']:.2f} (-1æœ€è´Ÿé¢ï¼Œ1æœ€æ­£é¢)
- è´Ÿé¢å¸–å­ï¼š{report['sentiment_analysis']['negative_posts']}
- æ­£é¢å¸–å­ï¼š{report['sentiment_analysis']['positive_posts']}
- ä¸­æ€§å¸–å­ï¼š{report['sentiment_analysis']['neutral_posts']}

## çƒ­é—¨è®¨è®ºï¼ˆäº’åŠ¨æœ€å¤šï¼‰
"""
        for i, post in enumerate(report['top_posts'][:5], 1):
            md_content += f"""
### {i}. {post['title']}
- Subreddit: r/{post['subreddit']}
- èµæ•°ï¼š{post['score']}ï¼Œè¯„è®ºæ•°ï¼š{post['num_comments']}
- ç—›ç‚¹ï¼š{', '.join(post['pain_points']) if post['pain_points'] else 'æ— æ˜ç¡®ç—›ç‚¹'}
- [æŸ¥çœ‹åŸå¸–]({post['url']})
"""
        
        if report['price_insights']:
            md_content += "\n## ä»·æ ¼æåŠ\n"
            for price, count in list(report['price_insights'].items())[:5]:
                md_content += f"- {price}: æåŠ{count}æ¬¡\n"
        
        with open(f'research_results/report_{timestamp}.md', 'w', encoding='utf-8') as f:
            f.write(md_content)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¼€å§‹Redditéœ€æ±‚ç ”ç©¶...")
    
    # åˆ›å»ºç ”ç©¶å™¨å®ä¾‹
    researcher = RedditResearcher()
    
    # 1. æœç´¢å¸–å­
    print("\nğŸ“ æ­£åœ¨æœç´¢ç›¸å…³å¸–å­...")
    posts_df = researcher.search_posts(days_back=30, limit_per_sub=50)
    print(f"âœ… æ‰¾åˆ° {len(posts_df)} ä¸ªç›¸å…³å¸–å­")
    
    # 2. è·å–çƒ­é—¨å¸–å­çš„è¯„è®º
    print("\nğŸ’¬ æ­£åœ¨åˆ†æè¯„è®º...")
    hot_posts = posts_df.nlargest(20, 'num_comments')['post_id'].tolist()
    comments_df = researcher.search_comments(hot_posts, limit=20)
    print(f"âœ… åˆ†æäº† {len(comments_df)} æ¡è¯„è®º")
    
    # 3. ç”ŸæˆæŠ¥å‘Š
    print("\nğŸ“Š æ­£åœ¨ç”ŸæˆæŠ¥å‘Š...")
    report = researcher.generate_report(posts_df, comments_df)
    
    # 4. ä¿å­˜ç»“æœ
    timestamp = researcher.save_results(posts_df, comments_df, report)
    
    print(f"\nâœ… ç ”ç©¶å®Œæˆï¼ç»“æœä¿å­˜åœ¨ research_results/ ç›®å½•")
    print(f"ğŸ“„ æŸ¥çœ‹æŠ¥å‘Šï¼šresearch_results/report_{timestamp}.md")
    
    # æ‰“å°å…³é”®å‘ç°
    print("\nğŸ¯ å…³é”®å‘ç°ï¼š")
    print(f"æœ€å¤§ç—›ç‚¹ï¼š{list(report['pain_points'].keys())[0] if report['pain_points'] else 'æ— '}")
    print(f"å¹³å‡æƒ…æ„Ÿï¼š{'è´Ÿé¢' if report['sentiment_analysis']['average_sentiment'] < -0.1 else 'æ­£é¢' if report['sentiment_analysis']['average_sentiment'] > 0.1 else 'ä¸­æ€§'}")
    
    if report['price_insights']:
        most_common_price = list(report['price_insights'].keys())[0]
        print(f"æœ€å¸¸æåŠçš„ä»·æ ¼ï¼š{most_common_price}")

if __name__ == "__main__":
    main()