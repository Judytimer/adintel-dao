#!/usr/bin/env python3
"""
Redditè®ºå›éœ€æ±‚ç ”ç©¶è„šæœ¬ - å®‰å…¨ç‰ˆæœ¬
åªè¿›è¡Œåªè¯»æ“ä½œï¼Œä¸å‘é€ä»»ä½•ç§ä¿¡ï¼Œç¬¦åˆRedditä½¿ç”¨è§„èŒƒ
"""

import praw
import pandas as pd
from datetime import datetime, timedelta
import re
from collections import Counter
import json
import time
from textblob import TextBlob
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å®‰å…¨è®¾ç½®
SAFE_MODE = True  # åªè¯»æ¨¡å¼
DELAY_SECONDS = 3  # æ¯æ¬¡è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
MAX_POSTS_PER_SUB = 30  # æ¯ä¸ªsubredditæœ€å¤šè¯»å–å¸–å­æ•°
MAX_COMMENTS_PER_POST = 20  # æ¯ä¸ªå¸–å­æœ€å¤šè¯»å–è¯„è®ºæ•°

class SafeRedditResearcher:
    def __init__(self):
        """åˆå§‹åŒ–Reddit APIè¿æ¥ - å®‰å…¨åªè¯»æ¨¡å¼"""
        print("ğŸ”’ åˆå§‹åŒ–å®‰å…¨æ¨¡å¼Redditç ”ç©¶å·¥å…·...")
        print("âœ… åªè¯»å–å…¬å¼€æ•°æ®")
        print("âœ… éµå®ˆAPIé€Ÿç‡é™åˆ¶")
        print("âŒ ä¸å‘é€ç§ä¿¡")
        print("âŒ ä¸è¿›è¡Œä»»ä½•å†™æ“ä½œ\n")
        
        # åªè¯»æ¨¡å¼è¿æ¥ - ä¸éœ€è¦ç”¨æˆ·åå¯†ç 
        try:
            self.reddit = praw.Reddit(
                client_id=os.getenv('REDDIT_CLIENT_ID', 'your_client_id'),
                client_secret=os.getenv('REDDIT_CLIENT_SECRET', 'your_client_secret'),
                user_agent='AdDAO Research Bot 1.0 (Read-Only, Educational Purpose)'
            )
            # æµ‹è¯•è¿æ¥
            test = self.reddit.subreddit('test').hot(limit=1)
            list(test)  # è§¦å‘å®é™…è¯·æ±‚
            print("âœ… Reddit APIè¿æ¥æˆåŠŸï¼ˆåªè¯»æ¨¡å¼ï¼‰\n")
        except Exception as e:
            print(f"âŒ è¿æ¥å¤±è´¥ï¼š{e}")
            print("è¯·æ£€æŸ¥ .env æ–‡ä»¶ä¸­çš„ REDDIT_CLIENT_ID å’Œ REDDIT_CLIENT_SECRET")
            raise
        
        # ç›®æ ‡subredditsï¼ˆæŒ‰ç›¸å…³åº¦æ’åºï¼‰
        self.target_subreddits = [
            'shopify',
            'ecommerce', 
            'FacebookAds',
            'dropshipping',
            'PPC'
        ]
        
        # ç—›ç‚¹å…³é”®è¯
        self.pain_keywords = {
            'competitor_spying': [
                'spy on competitors', 'competitor ads', 'what ads running',
                'competitive analysis', 'competitor research', 'spy tools',
                'see competitor ads', 'track competitors'
            ],
            'ad_cost': [
                'ads expensive', 'high cpc', 'rising costs', 'budget waste',
                'cpm increase', 'roas decrease', 'losing money', 'ad costs',
                'too expensive', 'burning money'
            ],
            'optimization': [
                'optimize ads', 'improve performance', 'better roas',
                'conversion rate', 'ad fatigue', 'creative ideas',
                'what works', 'best practices'
            ],
            'analytics': [
                'track metrics', 'understand data', 'analytics confusing',
                'which metrics', 'reporting', 'dashboard', 'measure roi'
            ],
            'targeting': [
                'audience targeting', 'find customers', 'interests',
                'lookalike', 'custom audience', 'retargeting', 'who to target'
            ]
        }
        
        # è§£å†³æ–¹æ¡ˆå…³é”®è¯
        self.solution_keywords = [
            'tool', 'software', 'app', 'platform', 'service',
            'recommend', 'using', 'tried', 'works well', 'helped me'
        ]
        
        # ä»·æ ¼ç›¸å…³å…³é”®è¯
        self.price_keywords = [
            'pay', 'cost', 'price', 'worth', 'budget', 'afford',
            'expensive', 'cheap', 'free', 'trial'
        ]
        
    def search_posts(self, days_back=30, limit_per_sub=None):
        """å®‰å…¨åœ°æœç´¢ç›¸å…³å¸–å­"""
        if limit_per_sub is None:
            limit_per_sub = MAX_POSTS_PER_SUB
            
        all_posts = []
        total_subs = len(self.target_subreddits)
        
        print(f"ğŸ“Š å¼€å§‹åˆ†æ {total_subs} ä¸ªç›¸å…³subreddits...")
        print(f"â° é¢„è®¡éœ€è¦ {total_subs * limit_per_sub * DELAY_SECONDS / 60:.1f} åˆ†é’Ÿ\n")
        
        for i, subreddit_name in enumerate(self.target_subreddits, 1):
            print(f"\n[{i}/{total_subs}] æ­£åœ¨åˆ†æ r/{subreddit_name}...")
            
            try:
                subreddit = self.reddit.subreddit(subreddit_name)
                posts_found = 0
                
                # åªæœç´¢çƒ­é—¨å’Œæ–°å¸–å­
                for sort_by in ['hot', 'new']:
                    if posts_found >= limit_per_sub:
                        break
                        
                    posts = subreddit.hot(limit=limit_per_sub) if sort_by == 'hot' else subreddit.new(limit=limit_per_sub)
                    
                    for post in posts:
                        if posts_found >= limit_per_sub:
                            break
                            
                        # å®‰å…¨å»¶è¿Ÿ
                        time.sleep(DELAY_SECONDS)
                        
                        # è¿‡æ»¤æ—¶é—´
                        post_time = datetime.fromtimestamp(post.created_utc)
                        if post_time < datetime.now() - timedelta(days=days_back):
                            continue
                        
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›¸å…³å…³é”®è¯
                        post_text = (post.title + ' ' + post.selftext).lower()
                        if not any(keyword in post_text for pain_list in self.pain_keywords.values() for keyword in pain_list):
                            continue
                        
                        # æå–å¸–å­ä¿¡æ¯
                        post_data = {
                            'subreddit': subreddit_name,
                            'post_id': post.id,
                            'title': post.title,
                            'selftext': post.selftext[:500],  # é™åˆ¶é•¿åº¦
                            'author': str(post.author) if post.author else '[deleted]',
                            'created_utc': post_time,
                            'score': post.score,
                            'num_comments': post.num_comments,
                            'url': f"https://reddit.com{post.permalink}",
                            'sort_by': sort_by
                        }
                        
                        # åˆ†æç—›ç‚¹
                        post_data['pain_points'] = self.analyze_pain_points(post_text)
                        
                        # æƒ…æ„Ÿåˆ†æ
                        post_data['sentiment'] = self.analyze_sentiment(post_text)
                        
                        # æå–ä»·æ ¼æåŠ
                        post_data['price_mentions'] = self.extract_price_mentions(post_text)
                        
                        all_posts.append(post_data)
                        posts_found += 1
                        
                        print(f"  âœ“ æ‰¾åˆ°ç›¸å…³å¸–å­: {post.title[:50]}...")
                
                print(f"  ğŸ“Œ ä» r/{subreddit_name} æ”¶é›†äº† {posts_found} ä¸ªç›¸å…³å¸–å­")
                
            except Exception as e:
                print(f"  âŒ é”™è¯¯: {e}")
                continue
        
        print(f"\nâœ… æ€»å…±æ”¶é›†äº† {len(all_posts)} ä¸ªç›¸å…³å¸–å­")
        return pd.DataFrame(all_posts)
    
    def search_comments(self, post_ids, limit=None):
        """å®‰å…¨åœ°è·å–å¸–å­è¯„è®º"""
        if limit is None:
            limit = min(len(post_ids), 10)  # é»˜è®¤åªåˆ†æ10ä¸ªå¸–å­çš„è¯„è®º
            
        all_comments = []
        print(f"\nğŸ’¬ å¼€å§‹åˆ†æå‰ {limit} ä¸ªçƒ­é—¨å¸–å­çš„è¯„è®º...")
        
        for i, post_id in enumerate(post_ids[:limit], 1):
            print(f"  [{i}/{limit}] åˆ†æå¸–å­è¯„è®º...")
            
            try:
                time.sleep(DELAY_SECONDS)  # å®‰å…¨å»¶è¿Ÿ
                
                submission = self.reddit.submission(id=post_id)
                submission.comments.replace_more(limit=0)  # ä¸å±•å¼€"æ›´å¤šè¯„è®º"
                
                comment_count = 0
                for comment in submission.comments.list()[:MAX_COMMENTS_PER_POST]:
                    if hasattr(comment, 'body'):
                        comment_data = {
                            'post_id': post_id,
                            'comment_id': comment.id,
                            'body': comment.body[:300],  # é™åˆ¶é•¿åº¦
                            'author': str(comment.author) if comment.author else '[deleted]',
                            'score': comment.score,
                            'created_utc': datetime.fromtimestamp(comment.created_utc)
                        }
                        
                        # æ£€æŸ¥æ˜¯å¦æåˆ°è§£å†³æ–¹æ¡ˆ
                        comment_data['mentions_solution'] = any(
                            keyword in comment.body.lower() 
                            for keyword in self.solution_keywords
                        )
                        
                        # æå–ä»·æ ¼
                        comment_data['price_mentions'] = self.extract_price_mentions(comment.body)
                        
                        all_comments.append(comment_data)
                        comment_count += 1
                
                print(f"    âœ“ æ”¶é›†äº† {comment_count} æ¡è¯„è®º")
                
            except Exception as e:
                print(f"    âŒ è·å–è¯„è®ºé”™è¯¯: {e}")
                continue
        
        return pd.DataFrame(all_comments)
    
    def analyze_pain_points(self, text):
        """åˆ†ææ–‡æœ¬ä¸­çš„ç—›ç‚¹"""
        text_lower = text.lower()
        found_pain_points = []
        
        for pain_type, keywords in self.pain_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                found_pain_points.append(pain_type)
                
        return found_pain_points
    
    def analyze_sentiment(self, text):
        """æƒ…æ„Ÿåˆ†æ"""
        try:
            blob = TextBlob(text[:500])  # é™åˆ¶åˆ†æé•¿åº¦
            return {
                'polarity': round(blob.sentiment.polarity, 2),
                'subjectivity': round(blob.sentiment.subjectivity, 2)
            }
        except:
            return {'polarity': 0, 'subjectivity': 0}
    
    def extract_price_mentions(self, text):
        """æå–ä»·æ ¼æåŠ"""
        # åŒ¹é…å„ç§ä»·æ ¼æ ¼å¼
        price_patterns = [
            r'\$\d+(?:\.\d{2})?(?:/(?:month|mo|year|yr))?',
            r'\d+\s*(?:dollars|usd|bucks)',
            r'(?:pay|spend|cost|budget)\s*\d+'
        ]
        
        prices = []
        for pattern in price_patterns:
            matches = re.findall(pattern, text.lower(), re.IGNORECASE)
            prices.extend(matches)
            
        return prices[:3]  # æœ€å¤šè¿”å›3ä¸ªä»·æ ¼æåŠ
    
    def generate_report(self, posts_df, comments_df=None):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        
        report = {
            'metadata': {
                'generated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'posts_analyzed': len(posts_df),
                'comments_analyzed': len(comments_df) if comments_df is not None else 0,
                'subreddits': posts_df['subreddit'].unique().tolist()
            },
            'pain_points': {},
            'sentiment_analysis': {},
            'top_discussions': [],
            'price_insights': {},
            'solution_mentions': {}
        }
        
        # ç—›ç‚¹ç»Ÿè®¡
        all_pain_points = []
        for pain_list in posts_df['pain_points']:
            all_pain_points.extend(pain_list)
        
        pain_counter = Counter(all_pain_points)
        report['pain_points'] = dict(pain_counter.most_common())
        
        # æƒ…æ„Ÿåˆ†æ
        sentiments = posts_df['sentiment'].apply(lambda x: x['polarity'])
        report['sentiment_analysis'] = {
            'average_sentiment': round(sentiments.mean(), 2),
            'very_negative': len(sentiments[sentiments < -0.5]),
            'negative': len(sentiments[(sentiments >= -0.5) & (sentiments < -0.1)]),
            'neutral': len(sentiments[(sentiments >= -0.1) & (sentiments <= 0.1)]),
            'positive': len(sentiments[(sentiments > 0.1) & (sentiments <= 0.5)]),
            'very_positive': len(sentiments[sentiments > 0.5])
        }
        
        # çƒ­é—¨è®¨è®º
        top_posts = posts_df.nlargest(10, 'num_comments')[
            ['title', 'subreddit', 'score', 'num_comments', 'url', 'pain_points', 'price_mentions']
        ].to_dict('records')
        report['top_discussions'] = top_posts[:5]
        
        # ä»·æ ¼æ´å¯Ÿ
        all_prices = []
        for price_list in posts_df['price_mentions']:
            all_prices.extend(price_list)
        
        if comments_df is not None and len(comments_df) > 0:
            for price_list in comments_df['price_mentions']:
                all_prices.extend(price_list)
        
        if all_prices:
            price_counter = Counter(all_prices)
            report['price_insights'] = dict(price_counter.most_common(10))
        
        # è§£å†³æ–¹æ¡ˆæåŠ
        if comments_df is not None and 'mentions_solution' in comments_df.columns:
            solution_comments = comments_df[comments_df['mentions_solution']]
            report['solution_mentions'] = {
                'total': len(solution_comments),
                'percentage': round(len(solution_comments) / len(comments_df) * 100, 1) if len(comments_df) > 0 else 0
            }
        
        return report
    
    def save_results(self, posts_df, comments_df, report):
        """ä¿å­˜ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # åˆ›å»ºç»“æœç›®å½•
        os.makedirs('research_results', exist_ok=True)
        
        # ä¿å­˜æ•°æ®
        posts_df.to_csv(f'research_results/posts_{timestamp}.csv', index=False)
        if comments_df is not None and len(comments_df) > 0:
            comments_df.to_csv(f'research_results/comments_{timestamp}.csv', index=False)
        
        # ä¿å­˜æŠ¥å‘Š
        with open(f'research_results/report_{timestamp}.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self.generate_markdown_report(report, timestamp)
        
        return timestamp
    
    def generate_markdown_report(self, report, timestamp):
        """ç”Ÿæˆæ˜“è¯»çš„MarkdownæŠ¥å‘Š"""
        md_content = f"""# Redditç ”ç©¶æŠ¥å‘Š - {timestamp}

## ğŸ“Š ç ”ç©¶æ¦‚è§ˆ
- **ç”Ÿæˆæ—¶é—´**: {report['metadata']['generated_at']}
- **åˆ†æå¸–å­æ•°**: {report['metadata']['posts_analyzed']}
- **åˆ†æè¯„è®ºæ•°**: {report['metadata']['comments_analyzed']}
- **è¦†ç›–subreddits**: {', '.join(['r/' + s for s in report['metadata']['subreddits']])}

## ğŸ¯ ç—›ç‚¹åˆ†æ

### ç—›ç‚¹æ’å
"""
        pain_map = {
            'competitor_spying': 'ğŸ” ç«å“ç›‘æµ‹',
            'ad_cost': 'ğŸ’° å¹¿å‘Šæˆæœ¬',
            'optimization': 'ğŸ“ˆ ä¼˜åŒ–éœ€æ±‚',
            'analytics': 'ğŸ“Š æ•°æ®åˆ†æ',
            'targeting': 'ğŸ¯ å—ä¼—å®šä½'
        }
        
        for pain, count in report['pain_points'].items():
            pain_label = pain_map.get(pain, pain)
            percentage = count / report['metadata']['posts_analyzed'] * 100
            md_content += f"- **{pain_label}**: {count}æ¬¡æåŠ ({percentage:.1f}%)\n"
        
        md_content += f"""

## ğŸ˜Š æƒ…æ„Ÿåˆ†æ
- **å¹³å‡æƒ…æ„Ÿå€¼**: {report['sentiment_analysis']['average_sentiment']} (-1æœ€è´Ÿé¢ï¼Œ1æœ€æ­£é¢)
- **éå¸¸è´Ÿé¢** (< -0.5): {report['sentiment_analysis']['very_negative']}ä¸ªå¸–å­
- **è´Ÿé¢** (-0.5 ~ -0.1): {report['sentiment_analysis']['negative']}ä¸ªå¸–å­
- **ä¸­æ€§** (-0.1 ~ 0.1): {report['sentiment_analysis']['neutral']}ä¸ªå¸–å­
- **æ­£é¢** (0.1 ~ 0.5): {report['sentiment_analysis']['positive']}ä¸ªå¸–å­
- **éå¸¸æ­£é¢** (> 0.5): {report['sentiment_analysis']['very_positive']}ä¸ªå¸–å­

## ğŸ”¥ çƒ­é—¨è®¨è®ºTOP 5
"""
        for i, post in enumerate(report['top_discussions'], 1):
            pain_labels = [pain_map.get(p, p) for p in post['pain_points']]
            md_content += f"""
### {i}. {post['title']}
- **Subreddit**: r/{post['subreddit']}
- **äº’åŠ¨**: {post['score']}èµï¼Œ{post['num_comments']}è¯„è®º
- **ç—›ç‚¹**: {', '.join(pain_labels) if pain_labels else 'æ— æ˜ç¡®ç—›ç‚¹'}
- **ä»·æ ¼æåŠ**: {', '.join(post['price_mentions']) if post['price_mentions'] else 'æ— '}
- **[æŸ¥çœ‹åŸå¸–]({post['url']})**
"""
        
        if report['price_insights']:
            md_content += "\n## ğŸ’µ ä»·æ ¼æ´å¯Ÿ\n\nç”¨æˆ·æåˆ°çš„ä»·æ ¼ç‚¹ï¼š\n"
            for price, count in list(report['price_insights'].items())[:10]:
                md_content += f"- `{price}`: æåŠ{count}æ¬¡\n"
        
        if report.get('solution_mentions'):
            md_content += f"""

## ğŸ› ï¸ è§£å†³æ–¹æ¡ˆè®¨è®º
- **æåŠè§£å†³æ–¹æ¡ˆçš„è¯„è®º**: {report['solution_mentions']['total']}æ¡
- **å æ€»è¯„è®ºæ¯”ä¾‹**: {report['solution_mentions']['percentage']}%
"""
        
        md_content += """

## ğŸš€ å…³é”®æ´å¯Ÿä¸å»ºè®®

### 1. ç”¨æˆ·æœ€å…³å¿ƒçš„é—®é¢˜
åŸºäºç—›ç‚¹åˆ†æï¼Œç”¨æˆ·æœ€å…³å¿ƒçš„å‰ä¸‰ä¸ªé—®é¢˜æ˜¯ï¼š
- ç«å“ç›‘æµ‹éœ€æ±‚å¼ºçƒˆï¼Œè¯´æ˜å¸‚åœºç¼ºä¹å¹³ä»·çš„ç«å“å¹¿å‘Šåˆ†æå·¥å…·
- å¹¿å‘Šæˆæœ¬ä¸Šå‡æ˜¯æ™®éç—›ç‚¹ï¼Œç”¨æˆ·éœ€è¦æ›´é«˜æ•ˆçš„æŠ•æ”¾ç­–ç•¥
- æ•°æ®åˆ†æå›°éš¾ï¼Œè¯´æ˜ç°æœ‰å·¥å…·è¿‡äºå¤æ‚

### 2. å¸‚åœºæœºä¼š
- ç›®æ ‡ä»·æ ¼åŒºé—´ï¼šåŸºäºä»·æ ¼æåŠåˆ†æï¼Œå»ºè®®å®šä»·åœ¨åˆç†åŒºé—´
- æ ¸å¿ƒåŠŸèƒ½ï¼šç«å“å¹¿å‘Šç›‘æµ‹ + æˆæœ¬ä¼˜åŒ–å»ºè®®
- å·®å¼‚åŒ–ï¼šç®€å•æ˜“ç”¨ï¼Œä¸“æ³¨ä¸­å°å•†å®¶

### 3. ä¸‹ä¸€æ­¥è¡ŒåŠ¨
1. æ·±å…¥ç ”ç©¶æ’åå‰5çš„è®¨è®ºå¸–
2. è”ç³»è¡¨ç°å‡ºå¼ºçƒˆéœ€æ±‚çš„ç”¨æˆ·
3. åŸºäºç—›ç‚¹ä¼˜å…ˆçº§è°ƒæ•´äº§å“åŠŸèƒ½

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š{report['metadata']['generated_at']}*
"""
        
        with open(f'research_results/report_{timestamp}.md', 'w', encoding='utf-8') as f:
            f.write(md_content)

def main():
    """ä¸»å‡½æ•° - å®‰å…¨æ¨¡å¼"""
    print("ğŸ” Redditå®‰å…¨è°ƒç ”å·¥å…· v1.0")
    print("=" * 50)
    
    try:
        # åˆ›å»ºç ”ç©¶å™¨å®ä¾‹
        researcher = SafeRedditResearcher()
        
        # 1. æœç´¢å¸–å­
        print("\nğŸ“ å¼€å§‹æœç´¢ç›¸å…³å¸–å­...")
        posts_df = researcher.search_posts(
            days_back=30,  # æœ€è¿‘30å¤©
            limit_per_sub=20  # æ¯ä¸ªsubæœ€å¤š20ä¸ªå¸–å­
        )
        
        if len(posts_df) == 0:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å¸–å­ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–APIé…ç½®")
            return
        
        print(f"\nâœ… æ‰¾åˆ° {len(posts_df)} ä¸ªç›¸å…³å¸–å­")
        
        # 2. è·å–çƒ­é—¨å¸–å­çš„è¯„è®ºï¼ˆå¯é€‰ï¼‰
        user_input = input("\næ˜¯å¦åˆ†æè¯„è®ºï¼Ÿè¿™ä¼šèŠ±è´¹æ›´å¤šæ—¶é—´ (y/n): ")
        comments_df = None
        
        if user_input.lower() == 'y':
            print("\nğŸ’¬ æ­£åœ¨åˆ†æè¯„è®º...")
            hot_posts = posts_df.nlargest(10, 'num_comments')['post_id'].tolist()
            comments_df = researcher.search_comments(hot_posts, limit=5)
            print(f"âœ… åˆ†æäº† {len(comments_df) if comments_df is not None else 0} æ¡è¯„è®º")
        
        # 3. ç”ŸæˆæŠ¥å‘Š
        print("\nğŸ“Š æ­£åœ¨ç”ŸæˆæŠ¥å‘Š...")
        report = researcher.generate_report(posts_df, comments_df)
        
        # 4. ä¿å­˜ç»“æœ
        timestamp = researcher.save_results(posts_df, comments_df, report)
        
        print(f"\nâœ… è°ƒç ”å®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨ research_results/ ç›®å½•")
        print(f"ğŸ“„ æŸ¥çœ‹æŠ¥å‘Šï¼šresearch_results/report_{timestamp}.md")
        
        # æ‰“å°å…³é”®å‘ç°
        print("\nğŸ¯ å…³é”®å‘ç°ï¼š")
        if report['pain_points']:
            top_pain = list(report['pain_points'].keys())[0]
            pain_map = {
                'competitor_spying': 'ç«å“ç›‘æµ‹',
                'ad_cost': 'å¹¿å‘Šæˆæœ¬',
                'optimization': 'ä¼˜åŒ–éœ€æ±‚',
                'analytics': 'æ•°æ®åˆ†æ',
                'targeting': 'å—ä¼—å®šä½'
            }
            print(f"  â€¢ æœ€å¤§ç—›ç‚¹ï¼š{pain_map.get(top_pain, top_pain)}")
        
        sentiment = report['sentiment_analysis']['average_sentiment']
        print(f"  â€¢ ç”¨æˆ·æƒ…ç»ªï¼š{'è´Ÿé¢' if sentiment < -0.1 else 'æ­£é¢' if sentiment > 0.1 else 'ä¸­æ€§'}")
        
        if report['price_insights']:
            print(f"  â€¢ ä»·æ ¼æåŠï¼š{list(report['price_insights'].keys())[0]}")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯ï¼š{e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()